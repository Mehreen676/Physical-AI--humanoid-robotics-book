---
sidebar_position: 1
---

# Integrated RAG Chatbot for Interactive Learning

## Introduction

This chapter describes the architecture, design, and implementation of an **Integrated RAG (Retrieval-Augmented Generation) Chatbot** embedded directly into this technical textbook. The chatbot transforms static content into an interactive learning experience, enabling readers to:

- Ask questions about any topic in the book and receive accurate, sourced answers.
- Query specific passages by selecting text and asking clarification questions.
- Access conversation history and resume previous learning sessions.
- Experience low-latency, cost-efficient responses using serverless cloud services.

The chatbot is production-ready, scalable, and adheres to strict accuracy constraints to prevent hallucination and maintain educational integrity.

---

## Section 1: System Architecture Diagram

### High-Level System Overview

The RAG Chatbot system consists of five primary layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Frontend (Docusaurus Textbook)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Chat UI Component (Sidebar/Modal)                     â”‚   â”‚
â”‚  â”‚ - Message display & input                             â”‚   â”‚
â”‚  â”‚ - Text selection capture & highlighting               â”‚   â”‚
â”‚  â”‚ - Session management UI                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ JavaScript SDK (chat-embed.js)                      â”‚    â”‚
â”‚  â”‚ - Event handlers for selected text                  â”‚    â”‚
â”‚  â”‚ - API calls to backend                              â”‚    â”‚
â”‚  â”‚ - Session storage (localStorage)                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTPS REST API
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FastAPI Backend (RAG Service)                     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ API Layer (Request/Response Routing)               â”‚    â”‚
â”‚  â”‚ - POST /query (full book queries)                  â”‚    â”‚
â”‚  â”‚ - POST /query-selected-text (passage queries)      â”‚    â”‚
â”‚  â”‚ - POST /ingest (admin: add/update chapters)        â”‚    â”‚
â”‚  â”‚ - GET /health (service status)                     â”‚    â”‚
â”‚  â”‚ - GET /sessions/{id} (conversation history)        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚               â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ RAG Pipeline Orchestrator                         â”‚    â”‚
â”‚  â”‚                                                    â”‚    â”‚
â”‚  â”‚ User Query Input                                  â”‚    â”‚
â”‚  â”‚    â†“                                               â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ 1. Retrieval Agent                          â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Embed query                            â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Query Qdrant for top-5 similar chunks  â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Return ranked results                  â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚               â”‚                                     â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ 2. Re-rank Agent (optional)                 â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Filter low-confidence results          â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Refine relevance ordering              â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚               â”‚                                     â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ 3. Generation Agent                         â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Construct prompt with context          â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Call OpenAI GPT-4o                     â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Format response with citations         â”‚   â”‚    â”‚
â”‚  â”‚ â”‚    - Enforce hallucination constraints      â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚               â”‚                                     â”‚    â”‚
â”‚  â”‚ Response with Sources                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         â”‚          â”‚          â”‚
        â–¼         â–¼          â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Qdrant â”‚  â”‚Neon      â”‚  â”‚ OpenAI  â”‚  â”‚ Sessions â”‚
    â”‚(Vectors)  â”‚Postgres  â”‚  â”‚  API    â”‚  â”‚(in Neon) â”‚
    â”‚        â”‚  â”‚(Metadata &  â”‚         â”‚  â”‚          â”‚
    â”‚        â”‚  â”‚ Chat Data)  â”‚         â”‚  â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Embeddings   Document &    LLM        Conversation
                 Session Data Generation   Persistence
```

### Component Responsibilities

#### Frontend Layer

**Docusaurus Chat UI**: Renders the interactive chat interface within the textbook. Features include:
- Message list displaying user queries and AI responses.
- Input field for typing questions.
- Text selection detection and "Ask about this" button.
- Session sidebar showing conversation history.
- Responsive design for desktop and mobile.

**JavaScript SDK (chat-embed.js)**: Lightweight library providing:
- Event listeners for text selection and highlighting.
- API client for backend endpoints with retry logic.
- Session persistence via localStorage.
- Error handling and user notifications.

#### API Layer

**FastAPI Backend**: RESTful service handling:
- Request validation and routing.
- Rate limiting and error handling.
- Structured logging for observability.
- Authentication for admin endpoints.

#### RAG Pipeline

**Three-Agent Architecture**:

1. **Retrieval Agent**: Embeds the user query and searches Qdrant for semantically similar chunks from the book. Returns top-5 candidates ranked by cosine similarity.

2. **Re-rank Agent** (optional): Filters low-confidence results and refines ordering using additional relevance metrics to improve quality of context passed to generation.

3. **Generation Agent**: Constructs a prompt combining retrieved context with strict RAG constraints, calls OpenAI's GPT-4o model, and formats the response with source citations.

#### Data Layer

**Qdrant Cloud** (Vector Store):
- Stores 1536-dimensional embeddings for all book chunks.
- Namespace-based isolation per book version (e.g., `book_v1.0`).
- Metadata tagging for filtering by chapter/module.
- Free tier: 1GB storage (sufficient for ~200k chunks).

**Neon Postgres** (Relational Database):
- Document metadata (chapter, section, hash).
- Chat sessions and message history.
- User preferences (optional).
- Free tier: 3GB storage.

**OpenAI API**:
- `text-embedding-3-small`: Generates embeddings (minimal cost).
- `gpt-4o`: Generates responses (with fallback to gpt-3.5-turbo).

---

## Section 2: RAG Pipeline Explanation (Step-by-Step)

### The Complete Query Flow

#### Step 1: User Submits Query or Selects Text

**Full Book Mode**: User types a question in the chat input.
```
User: "What are the key differences between ROS 1 and ROS 2?"
```

**Selected Text Mode**: User highlights a passage and clicks "Ask about this".
```
Selected Text: "ROS 2 is a complete rewrite of ROS 1, introducing..."
User: "Can you summarize this?"
```

#### Step 2: Input Validation & Session Creation

The backend validates the request:
- Query length: â‰¤ 500 characters.
- Selected text length: â‰¤ 10,000 characters (truncated if needed).
- Session ID: Created if not present.

```json
Request Payload:
{
  "query": "What are the key differences between ROS 1 and ROS 2?",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "book_version": "v1.0",
  "mode": "full_book"
}
```

#### Step 3: Query Embedding

The system embeds the user query using OpenAI's `text-embedding-3-small` model:

```python
# Backend code (pseudocode)
from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)
embedding = client.embeddings.create(
    model="text-embedding-3-small",
    input=user_query
).data[0].embedding
# embedding: list of 1536 floating-point values
```

**Cost**: ~$0.00001 per query (negligible for typical usage).

#### Step 4: Semantic Search in Qdrant

The embedded query is used to search Qdrant for similar chunks:

```python
# Retrieve top-5 chunks by cosine similarity
from qdrant_client import QdrantClient

client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY
)

results = client.search(
    collection_name="chapters",
    query_vector=embedding,
    limit=5,
    query_filter={  # Optional: filter by chapter
        "key": "chapter",
        "match": {"value": "Module 1: ROS 2 Fundamentals"}
    }
)

# results: [{id, score, payload}, ...]
# score: cosine similarity (0-1); higher = more relevant
```

**Typical Results** (example):
```
1. Score: 0.92 | Chapter: Module 1 | Section: ROS 2 vs ROS 1 | Excerpt: "Key differences..."
2. Score: 0.88 | Chapter: Module 1 | Section: ROS 2 Architecture | Excerpt: "ROS 2 introduces..."
3. Score: 0.85 | Chapter: Module 1 | Section: Middleware | Excerpt: "Unlike ROS 1..."
4. Score: 0.79 | Chapter: Module 2 | Section: Simulation Setup | Excerpt: "ROS 2 ecosystem..."
5. Score: 0.74 | Chapter: Appendix | Section: Resources | Excerpt: "ROS 2 documentation..."
```

#### Step 5: Re-ranking (Optional)

If enabled, a re-ranking step filters low-confidence results and refines ordering:

```python
# Filter results with score < 0.75
ranked_results = [r for r in results if r['score'] >= 0.75]

# Option: Use cross-encoder model for additional refinement
# This step is optional and improves quality at the cost of latency
```

#### Step 6: Context Construction

The top-k chunks are concatenated to form the retrieval context:

```
Retrieved Context:
---
Chapter: Module 1 - ROS 2 Fundamentals
Section: ROS 2 vs ROS 1

ROS 2 is a complete rewrite of ROS 1, introducing several key improvements:

1. Middleware Abstraction: ROS 2 abstracts the communication middleware, supporting DDS
   (Data Distribution Service) by default, enabling distributed systems and edge computing.

2. Type Safety: ROS 2 includes built-in type checking and serialization, reducing errors.

3. Real-Time Capabilities: ROS 2 supports quality of service (QoS) settings, enabling
   reliable, deterministic communication for real-time robotics applications.

[Additional chunks...]
---
```

#### Step 7: Prompt Construction with Constraints

A carefully engineered prompt is constructed to enforce RAG constraints:

```
System Prompt:
"You are an expert AI tutor for a technical textbook on Physical AI and Humanoid Robotics.
Answer questions ONLY based on the provided context from the textbook.

CRITICAL CONSTRAINTS:
1. ONLY use information from the retrieved context below.
2. If the answer is NOT in the context, respond: 'I don't have information about that in the textbook.'
3. Always cite the chapter and section of your sources.
4. Be concise and educational.
5. Do NOT make up or assume information not in the context."

User Prompt:
"Context from the textbook:
{retrieved_context}

Question: {user_query}

Answer (cite sources):"
```

#### Step 8: LLM Generation

OpenAI's GPT-4o model generates a response:

```python
from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.3,  # Low creativity to reduce hallucination
    max_tokens=500
)

answer = response.choices[0].message.content
```

**Temperature=0.3** ensures deterministic, focused responses based on retrieved context.

#### Step 9: Response Formatting & Citation

The response is formatted with inline citations:

```
Generated Response:
"ROS 2 and ROS 1 differ in several key ways (Module 1 - ROS 2 Fundamentals, Section: ROS 2 vs ROS 1):

1. **Middleware Abstraction**: ROS 2 abstracts the communication middleware, supporting DDS
   by default, enabling distributed systems and edge computing.

2. **Type Safety**: ROS 2 includes built-in type checking and serialization, reducing errors.

3. **Real-Time Capabilities**: ROS 2 supports QoS settings for reliable, deterministic
   communication in real-time robotics applications.

These improvements make ROS 2 more suitable for production robotics and safety-critical systems."
```

#### Step 10: Session Persistence

The message is logged to the database:

```python
# Store in Neon Postgres
new_message = Message(
    message_id=uuid.uuid4(),
    session_id=session_id,
    user_message=user_query,
    assistant_response=answer,
    source_chunk_ids=[r['id'] for r in results],
    mode="full_book",
    created_at=datetime.utcnow()
)
db.add(new_message)
db.commit()
```

#### Step 11: Response Delivery

The response is returned to the frontend with metadata:

```json
{
  "response": "ROS 2 and ROS 1 differ in several key ways...",
  "sources": [
    {
      "chapter": "Module 1: ROS 2 Fundamentals",
      "section": "ROS 2 vs ROS 1",
      "excerpt": "ROS 2 is a complete rewrite..."
    }
  ],
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "latency_ms": 4200,
  "confidence": 0.92
}
```

#### Step 12: UI Display

The frontend renders the response with sources and options to continue the conversation:

```
Assistant: "ROS 2 and ROS 1 differ in several key ways..."

ğŸ“š Sources:
  - Module 1: ROS 2 Fundamentals â†’ ROS 2 vs ROS 1

[Latency: 4.2s | Confidence: 92%]

ğŸ’¬ Follow up with more questions...
```

### Selected-Text Mode Flow

For selected-text queries, the flow is identical except:

1. **Step 3**: Embed selected text instead of full context.
2. **Step 4**: Use selected text as sole context (no Qdrant query).
3. **Step 7**: Enforce stricter constraint in prompt:

```
"Answer ONLY based on this selected passage. If the answer is NOT in the
selected text, respond: 'The selected text does not contain the answer.'"
```

4. **Step 9**: Validate response uses only selected text (semantic check).

### Hallucination Prevention Mechanisms

The pipeline includes multiple safeguards:

1. **Prompt Engineering**: System prompt explicitly forbids making up information.
2. **Context Constraint**: LLM sees only retrieved chunks; no external knowledge access.
3. **Temperature Setting**: Low temperature (0.3) reduces creativity.
4. **Confidence Scoring**: Semantic similarity scores indicate answer reliability.
5. **Fallback Response**: Out-of-scope queries return standard fallback message.
6. **Server-Side Validation**: Response semantic similarity to context is checked.

---

## Section 3: Database Schema

### Data Models

#### Document Metadata (Neon Postgres)

Stores information about book chapters and chunks:

```sql
CREATE TABLE documents (
    doc_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    book_version VARCHAR(10) NOT NULL,
    chapter VARCHAR(100) NOT NULL,
    section VARCHAR(100),
    doc_name VARCHAR(200),
    content_hash VARCHAR(64) UNIQUE NOT NULL,
    chunk_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_documents_book (book_version),
    INDEX idx_documents_hash (content_hash)
);
```

Example Data:
```
doc_id: 550e8400-e29b-41d4-a716-446655440000
book_version: v1.0
chapter: Module 1: ROS 2 Fundamentals
section: ROS 2 vs ROS 1
doc_name: module-1-ros2.md
content_hash: abc123def456...
chunk_count: 8
```

#### Chat Sessions (Neon Postgres)

Manages user conversation sessions:

```sql
CREATE TABLE sessions (
    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(100),
    book_version VARCHAR(10) DEFAULT 'v1.0',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_sessions_user (user_id),
    INDEX idx_sessions_created (created_at)
);
```

Example Data:
```
session_id: 660e8400-e29b-41d4-a716-446655440001
user_id: (null for anonymous)
book_version: v1.0
created_at: 2025-12-16 10:00:00
updated_at: 2025-12-16 10:15:00
```

#### Chat Messages (Neon Postgres)

Stores conversation history:

```sql
CREATE TABLE messages (
    message_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID NOT NULL REFERENCES sessions(session_id) ON DELETE CASCADE,
    user_message TEXT NOT NULL,
    assistant_response TEXT NOT NULL,
    source_chunk_ids JSONB,  -- array of Qdrant point IDs
    mode VARCHAR(20),  -- 'full_book' or 'selected_text'
    selected_text TEXT,  -- if mode='selected_text'
    latency_ms INTEGER,
    confidence NUMERIC(3,2),  -- 0.0-1.0
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (session_id) REFERENCES sessions(session_id),
    INDEX idx_messages_session (session_id),
    INDEX idx_messages_created (created_at)
);
```

Example Data:
```
message_id: 770e8400-e29b-41d4-a716-446655440002
session_id: 660e8400-e29b-41d4-a716-446655440001
user_message: "What are the key differences between ROS 1 and ROS 2?"
assistant_response: "ROS 2 and ROS 1 differ in several key ways..."
source_chunk_ids: [1001, 1002, 1003]
mode: "full_book"
latency_ms: 4200
confidence: 0.92
```

#### Qdrant Vector Storage

Stores embeddings and chunk metadata (cloud-managed; no SQL schema):

```json
Collection: "book_v1.0_chapters"
Point Structure:
{
    "id": 1001,
    "vector": [0.123, -0.456, 0.789, ...],  // 1536 dimensions
    "payload": {
        "doc_id": "550e8400-e29b-41d4-a716-446655440000",
        "chapter": "Module 1: ROS 2 Fundamentals",
        "section": "ROS 2 vs ROS 1",
        "chunk_id": 0,
        "content": "ROS 2 is a complete rewrite of ROS 1...",
        "content_hash": "abc123def456...",
        "created_at": "2025-12-16T10:00:00Z"
    }
}
```

### Data Relationships

```
documents (1) â”€â”€â”€â”€â”€â”€â”€â”€ (many) qdrant_points (vectors)
                      â””â”€ indexed by content_hash & doc_id

sessions (1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (many) messages
                       â””â”€ indexed by session_id

messages (n) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€ documents (for source_chunk_ids)
                      â””â”€ qdrant_points (via source_chunk_ids)
```

### Data Retention & Cleanup

- **Documents**: Retained indefinitely (reference data).
- **Sessions**: Retained for 1 year; archive older sessions to cold storage.
- **Qdrant Vectors**: Refresh monthly on content updates; old vectors auto-replaced.

---

## Section 4: Agent Roles and Responsibilities

### The Three-Agent Architecture

The RAG pipeline orchestrates three distinct agents, each with specific responsibilities:

#### Agent 1: Retrieval Agent

**Primary Role**: Semantic search and context discovery.

**Responsibilities**:
1. Embed user query to 1536-dim vector.
2. Query Qdrant for top-k (k=5) semantically similar chunks.
3. Apply metadata filters if specified (chapter, module).
4. Rank results by cosine similarity score.
5. Return chunks with scores, chapter, section, and excerpt.

**Constraints**:
- Retrieval latency â‰¤ 500ms (p95).
- Confidence threshold: Score â‰¥ 0.6 (configurable).
- Max context window: Concatenate up to 2000 tokens.

**Error Handling**:
- No results found: Return empty context (downstream agent handles).
- Qdrant unavailable: Escalate error; suggest retry.

**Example Pseudocode**:
```python
class RetrievalAgent:
    def retrieve(self, query: str, filters: dict = None) -> List[Chunk]:
        # Step 1: Embed query
        embedding = embed_text(query)

        # Step 2: Query Qdrant
        results = qdrant_client.search(
            collection="chapters",
            query_vector=embedding,
            limit=5,
            query_filter=filters
        )

        # Step 3: Format results
        chunks = [
            Chunk(
                id=r.id,
                content=r.payload['content'],
                chapter=r.payload['chapter'],
                section=r.payload['section'],
                score=r.score
            )
            for r in results
        ]

        return chunks
```

---

#### Agent 2: Re-rank Agent (Optional)

**Primary Role**: Result refinement and quality filtering.

**Responsibilities**:
1. Filter results with low confidence (score < 0.6).
2. Optionally apply cross-encoder model for deeper semantic analysis.
3. Reorder results by relevance.
4. Truncate context if exceeds token limit.

**Constraints**:
- Re-ranking latency â‰¤ 200ms (acceptable overhead).
- Maintain top-k results (avoid over-filtering).

**Error Handling**:
- If all results filtered: Use original results; log warning.

**Example Pseudocode**:
```python
class ReRankAgent:
    def rerank(self, query: str, chunks: List[Chunk]) -> List[Chunk]:
        # Step 1: Filter low-confidence
        filtered = [c for c in chunks if c.score >= 0.6]

        # Step 2: Optional cross-encoder re-ranking
        if len(filtered) > 0:
            # Advanced: Use cross-encoder for deeper relevance
            # scores = cross_encoder.predict([[query, c.content] for c in filtered])
            # filtered.sort(key=lambda c: scores[c.id], reverse=True)
            pass

        # Step 3: Concatenate and truncate to token limit
        context = ""
        tokens = 0
        for chunk in filtered:
            chunk_tokens = count_tokens(chunk.content)
            if tokens + chunk_tokens <= 2000:
                context += f"\n[{chunk.chapter}] {chunk.content}"
                tokens += chunk_tokens

        return (context, len(filtered))
```

---

#### Agent 3: Generation Agent

**Primary Role**: Answer synthesis with factual grounding and citation.

**Responsibilities**:
1. Construct RAG-constrained prompt (system + user messages).
2. Call OpenAI API with retrieved context.
3. Extract and format response.
4. Add source citations.
5. Validate response against context (hallucination check).
6. Implement fallback responses for out-of-scope queries.

**Constraints**:
- Generation latency â‰¤ 5s (p95).
- Temperature = 0.3 (low creativity).
- Max tokens = 500.
- Hallucination rate < 5%.

**Error Handling**:
- OpenAI API failure: Fallback to gpt-3.5-turbo or cached response.
- Rate limit hit: Queue query for retry; notify user.

**Example Pseudocode**:
```python
class GenerationAgent:
    SYSTEM_PROMPT = """You are an expert AI tutor for a technical textbook.
Answer ONLY based on the provided context. If the answer is NOT in the context,
respond: 'I don't have information about that in the textbook.' Always cite sources."""

    def generate(self, query: str, context: str, selected_text: bool = False) -> str:
        # Step 1: Construct prompt
        if selected_text:
            constraint = "Answer ONLY based on this selected text passage."
        else:
            constraint = "Answer ONLY based on the retrieved context below."

        user_prompt = f"{constraint}\n\nContext:\n{context}\n\nQuestion: {query}"

        # Step 2: Call OpenAI
        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            answer = response.choices[0].message.content
        except RateLimitError:
            # Fallback
            answer = self._fallback_response(query)

        # Step 3: Add citations
        formatted_answer = self._format_with_citations(answer, context)

        # Step 4: Validate (selected-text mode)
        if selected_text:
            if not self._is_in_context(answer, context):
                formatted_answer = "The selected text does not contain the answer."

        return formatted_answer
```

---

### Agent Communication & Orchestration

The three agents form a **pipeline** orchestrated by the main FastAPI endpoint:

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Validation                 â”‚
â”‚  - Check query length              â”‚
â”‚  - Create/fetch session            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieval Agent                  â”‚
â”‚  - Embed query                     â”‚
â”‚  - Search Qdrant                   â”‚
â”‚  â†’ Returns: chunks with scores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Re-rank Agent (optional)          â”‚
â”‚  - Filter low-confidence            â”‚
â”‚  - Reorder by relevance            â”‚
â”‚  â†’ Returns: filtered chunks        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generation Agent                 â”‚
â”‚  - Construct prompt                â”‚
â”‚  - Call OpenAI GPT-4o              â”‚
â”‚  - Format with citations           â”‚
â”‚  â†’ Returns: formatted response     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session Persistence              â”‚
â”‚  - Store message in Neon           â”‚
â”‚  - Update session timestamp        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
        Response to User
```

---

## Section 5: Prompt Templates

### System Prompt (Full Book Mode)

The system prompt establishes the chatbot's role and constraints:

```
You are an expert AI tutor for a technical textbook titled "Physical AI & Humanoid Robotics."
Your role is to help readers understand complex concepts, answer questions, and guide their learning.

CRITICAL CONSTRAINTS:
1. Answer ONLY based on the retrieved context provided below.
2. Do NOT make up, assume, or infer information not explicitly in the context.
3. If the answer is NOT in the provided context, respond verbatim:
   "I don't have information about that in the textbook."
4. Always cite the chapter and section where your answer comes from.
5. Be concise, educational, and accurate.
6. For multi-part questions, address each part separately.

TONE & STYLE:
- Professional yet accessible.
- Use examples and analogies when helpful.
- Break complex concepts into digestible parts.
- Encourage further learning and curiosity.

CITATION FORMAT:
When citing, use this format:
  (Chapter/Section Name)

Example: "ROS 2 introduces middleware abstraction (Module 1: ROS 2 Fundamentals, Section: ROS 2 vs ROS 1)."
```

### System Prompt (Selected-Text Mode)

For selected-text queries, the constraint is stricter:

```
You are an expert AI tutor for a technical textbook.

CRITICAL: You MUST answer ONLY based on the selected passage provided below.
Do NOT reference any external knowledge or other parts of the textbook.

If the answer is NOT in the selected passage, respond verbatim:
  "The selected text does not contain the answer."

Otherwise, answer concisely based solely on what the passage says.
```

### User Prompt (Full Book Mode)

```
Context from the textbook:
---
{RETRIEVED_CHUNKS_WITH_CITATIONS}
---

Question: {USER_QUERY}

Please provide a clear, accurate answer based on the above context.
If the context doesn't contain the answer, state so explicitly.
```

### User Prompt (Selected-Text Mode)

```
Selected passage:
---
{SELECTED_TEXT}
---

Question: {USER_QUERY}

Answer based ONLY on the selected passage. If the answer is not in the passage,
respond: "The selected text does not contain the answer."
```

### Example Interaction (Full Book Mode)

**User Query**:
```
What are the advantages of ROS 2 over ROS 1?
```

**Constructed Prompt (system + user)**:
```
System: [Full Book System Prompt]

User: Context from the textbook:
---
[Chapter: Module 1: ROS 2 Fundamentals]
Section: ROS 2 vs ROS 1

ROS 2 is a complete rewrite of ROS 1 with several key improvements:

1. **Middleware Abstraction**: ROS 2 abstracts the communication middleware, supporting
   DDS (Data Distribution Service) by default. This enables distributed systems, edge
   computing, and integration with various platforms.

2. **Type Safety**: ROS 2 includes built-in type checking and serialization, reducing
   errors and improving robustness.

3. **Quality of Service (QoS)**: ROS 2 supports configurable QoS settings for reliable,
   deterministic communicationâ€”essential for real-time robotics.

4. **Better Security**: ROS 2 includes encryption, authentication, and access control
   capabilities for secure distributed systems.
---

Question: What are the advantages of ROS 2 over ROS 1?

Please provide a clear, accurate answer based on the above context.
```

**Generated Response** (from GPT-4o):
```
ROS 2 offers several significant advantages over ROS 1 (Module 1: ROS 2 Fundamentals, ROS 2 vs ROS 1):

1. **Middleware Abstraction**: Unlike ROS 1, which is tightly coupled to a specific middleware,
   ROS 2 abstracts this layer and defaults to DDS. This enables seamless integration with
   distributed systems and edge computing platforms.

2. **Type Safety**: ROS 2 includes built-in type checking and serialization, making it more
   robust and reducing common errors compared to ROS 1's loosely-typed message system.

3. **Real-Time Capabilities**: ROS 2's QoS framework provides configurable reliability and
   deterministic communication, making it suitable for time-critical robotics applications
   where ROS 1 falls short.

4. **Enhanced Security**: ROS 2 provides encryption, authentication, and access controlâ€”
   critical for secure multi-robot and distributed systems where ROS 1 lacks these features.

These improvements position ROS 2 as a production-ready framework for modern robotics.
```

---

## Section 6: FastAPI Endpoint Examples

### Endpoint 1: POST /query (Full Book Mode)

**Purpose**: Answer questions using full book content.

**Request**:
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the purpose of ROS 2 middleware abstraction?",
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "book_version": "v1.0",
    "filters": {
      "chapter": "Module 1"
    }
  }'
```

**Response** (200 OK):
```json
{
  "response": "ROS 2 middleware abstraction serves several purposes (Module 1: ROS 2 Fundamentals). First, it decouples the communication protocol from the application layer, enabling support for multiple middleware systems like DDS, MQTT, and others. This flexibility allows developers to choose the middleware that best fits their use case. Second, it facilitates distributed systems and edge computing by allowing nodes to communicate across networks with built-in discovery and inter-operability. Finally, the abstraction enables better security, real-time capabilities, and quality-of-service settings that ROS 1 lacked.",
  "sources": [
    {
      "chapter": "Module 1: ROS 2 Fundamentals",
      "section": "Middleware Abstraction",
      "excerpt": "ROS 2 abstracts the communication middleware, supporting DDS by default..."
    }
  ],
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "latency_ms": 4200,
  "confidence": 0.91
}
```

**Response** (No relevant context):
```json
{
  "response": "I don't have information about that in the textbook.",
  "sources": [],
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "latency_ms": 1800,
  "confidence": 0.0
}
```

**Response** (400 Bad Request):
```json
{
  "error": "Invalid request",
  "message": "query cannot be empty",
  "status": 400
}
```

**Response** (429 Rate Limited):
```json
{
  "error": "Rate limit exceeded",
  "message": "10 queries per minute per session limit reached",
  "retry_after_seconds": 60,
  "status": 429
}
```

---

### Endpoint 2: POST /query-selected-text (Selected-Text Mode)

**Purpose**: Answer questions using only user-selected text.

**Request**:
```bash
curl -X POST http://localhost:8000/query-selected-text \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What does this passage say about ROS 2?",
    "selected_text": "ROS 2 is a complete rewrite of ROS 1 with several key improvements: middleware abstraction, type safety, and quality of service support.",
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "book_version": "v1.0"
  }'
```

**Response** (Answer in selected text):
```json
{
  "response": "According to the selected passage, ROS 2 is a complete rewrite of ROS 1 that introduces three key improvements: (1) middleware abstraction, (2) type safety, and (3) quality of service support.",
  "in_selected_text": true,
  "sources": [
    {
      "type": "selected_text",
      "excerpt": "ROS 2 is a complete rewrite of ROS 1 with several key improvements..."
    }
  ],
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "latency_ms": 3500
}
```

**Response** (Answer NOT in selected text):
```json
{
  "response": "The selected text does not contain the answer.",
  "in_selected_text": false,
  "sources": [],
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "latency_ms": 2100
}
```

---

### Endpoint 3: POST /ingest (Admin: Add/Update Chapters)

**Purpose**: Ingest or update book chapters (admin-only endpoint).

**Request**:
```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secure_api_key" \
  -d '{
    "book_version": "v1.0",
    "chapters": [
      {
        "chapter": "Module 1",
        "section": "ROS 2 Fundamentals",
        "doc_name": "module-1-ros2.md",
        "content": "# ROS 2 Fundamentals\n\n## What is ROS 2?\n\nROS 2 is a middleware for robot software development..."
      }
    ],
    "skip_duplicates": true
  }'
```

**Response** (200 OK):
```json
{
  "ingested": 8,
  "skipped_duplicates": 2,
  "failed": 0,
  "vectors_stored": 8,
  "namespaces": ["book_v1.0"],
  "total_cost": "$0.002"
}
```

**Response** (401 Unauthorized):
```json
{
  "error": "Unauthorized",
  "message": "Invalid or missing API key",
  "status": 401
}
```

---

### Endpoint 4: GET /health (Service Status)

**Purpose**: Health check for monitoring and load balancing.

**Request**:
```bash
curl http://localhost:8000/health
```

**Response** (200 OK):
```json
{
  "status": "healthy",
  "components": {
    "qdrant": {
      "status": "ok",
      "latency_ms": 45
    },
    "neon": {
      "status": "ok",
      "latency_ms": 23
    },
    "openai": {
      "status": "ok",
      "latency_ms": 150
    }
  },
  "version": "1.0.0",
  "uptime_seconds": 86400
}
```

**Response** (503 Service Unavailable):
```json
{
  "status": "degraded",
  "components": {
    "qdrant": {
      "status": "error",
      "error": "Connection timeout"
    }
  }
}
```

---

### Endpoint 5: GET /sessions/{session_id} (Conversation History)

**Purpose**: Retrieve a specific conversation session and its history.

**Request**:
```bash
curl http://localhost:8000/sessions/550e8400-e29b-41d4-a716-446655440000
```

**Response** (200 OK):
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "created_at": "2025-12-16T10:00:00Z",
  "updated_at": "2025-12-16T10:15:30Z",
  "message_count": 5,
  "messages": [
    {
      "message_id": "msg-001",
      "user_message": "What is ROS 2?",
      "assistant_response": "ROS 2 is a flexible middleware for robot software development...",
      "mode": "full_book",
      "latency_ms": 4200,
      "confidence": 0.89,
      "created_at": "2025-12-16T10:00:10Z"
    },
    {
      "message_id": "msg-002",
      "user_message": "What are its advantages over ROS 1?",
      "assistant_response": "ROS 2 offers several advantages...",
      "mode": "full_book",
      "latency_ms": 3900,
      "confidence": 0.91,
      "created_at": "2025-12-16T10:00:25Z"
    }
  ]
}
```

**Response** (404 Not Found):
```json
{
  "error": "Session not found",
  "session_id": "invalid-session-id",
  "status": 404
}
```

---

## Section 7: Selected Text QA Logic

### Client-Side Text Selection

When a user selects text in the book, the JavaScript SDK captures and highlights it:

```javascript
// chat-embed.js (JavaScript SDK)

class RagChatbot {
  init(config) {
    this.config = config;
    this.selectedText = "";

    // Listen for text selection
    document.addEventListener('selectionchange', () => {
      const selection = window.getSelection();
      if (selection.toString().length > 0) {
        this.selectedText = selection.toString();
        this._highlightSelection();
        this._showAskButton();
      }
    });
  }

  _highlightSelection() {
    const selection = window.getSelection();
    const range = selection.getRangeAt(0);
    const span = document.createElement('span');
    span.style.backgroundColor = '#FFFF00';
    span.style.cursor = 'pointer';
    range.surroundContents(span);
  }

  _showAskButton() {
    const button = document.createElement('button');
    button.textContent = 'ğŸ’¬ Ask about this';
    button.onclick = () => this._querySelectedText();
    document.body.appendChild(button);
  }
}
```

### Selected-Text Validation (Server-Side)

The backend validates that responses use only the selected text:

```python
# FastAPI backend: rag_service.py

from sentence_transformers import SentenceTransformer, util

class SelectedTextValidator:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def is_response_in_context(self, response: str, selected_text: str, threshold=0.7):
        """
        Validate that response semantically aligns with selected text.

        Args:
            response: Generated response from LLM.
            selected_text: User-selected passage.
            threshold: Semantic similarity threshold (0-1).

        Returns:
            True if response is grounded in selected text; False otherwise.
        """
        # Embed both response and selected text
        response_embedding = self.model.encode(response, convert_to_tensor=True)
        context_embedding = self.model.encode(selected_text, convert_to_tensor=True)

        # Compute cosine similarity
        similarity = util.pytorch_cos_sim(response_embedding, context_embedding).item()

        # Check if concepts in response appear in selected text
        response_keywords = self._extract_keywords(response)
        context_keywords = self._extract_keywords(selected_text)

        keyword_overlap = len(set(response_keywords) & set(context_keywords)) / len(response_keywords)

        # Response is valid if similarity is high OR keyword overlap is substantial
        is_valid = (similarity >= threshold) or (keyword_overlap >= 0.5)

        return is_valid

    def _extract_keywords(self, text: str) -> List[str]:
        """Extract key concepts from text (simplified)."""
        # Simple approach: extract nouns and important terms
        # In production, use NLP library (spaCy, NLTK)
        words = text.lower().split()
        stopwords = {'the', 'a', 'is', 'and', 'or', 'of', 'in', 'to', 'for'}
        return [w for w in words if w not in stopwords and len(w) > 3]
```

### Query-Selected-Text Endpoint Logic

```python
# FastAPI: POST /query-selected-text

@app.post("/query-selected-text")
async def query_selected_text(request: QuerySelectedTextRequest):
    """
    Process query using ONLY selected text.

    Request:
        query: User question
        selected_text: User-selected passage (max 10k chars)
        session_id: Conversation session

    Response:
        response: Answer or fallback message
        in_selected_text: Boolean indicating if answer is in text
    """

    # Step 1: Validate input
    if not request.query or not request.selected_text:
        raise ValueError("query and selected_text are required")

    # Truncate if oversized
    if len(request.selected_text) > 10000:
        request.selected_text = request.selected_text[:10000]
        user_notification = "Selected text truncated to 10,000 characters."

    # Step 2: Get or create session
    session = get_or_create_session(request.session_id)

    # Step 3: Generate response using ONLY selected text
    try:
        response = generation_agent.generate(
            query=request.query,
            context=request.selected_text,
            selected_text=True  # Enforce strict constraint
        )
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        response = "The selected text does not contain the answer."
        in_selected_text = False
    else:
        # Step 4: Validate response is grounded in selected text
        validator = SelectedTextValidator()
        in_selected_text = validator.is_response_in_context(
            response, request.selected_text, threshold=0.7
        )

        if not in_selected_text:
            response = "The selected text does not contain the answer."

    # Step 5: Store message in session
    message = Message(
        session_id=session.session_id,
        user_message=request.query,
        assistant_response=response,
        mode="selected_text",
        selected_text=request.selected_text[:500],  # Store excerpt
        source_chunk_ids=None
    )
    db.add(message)
    db.commit()

    # Step 6: Return response
    return {
        "response": response,
        "in_selected_text": in_selected_text,
        "sources": [{"type": "selected_text", "excerpt": request.selected_text[:200]}],
        "session_id": session.session_id,
        "latency_ms": round(time.time() - start_time) * 1000
    }
```

### Handling Edge Cases

**Case 1: Very Long Selected Text**

```python
if len(selected_text) > 2000:
    # Truncate intelligently to 2000 tokens
    selected_text = truncate_to_tokens(selected_text, max_tokens=2000)
    user_warning = "Selected text is long; truncated to 2000 tokens. Results may be incomplete."
```

**Case 2: Selected Text with Code/Tables**

```python
# Pre-process selected text to handle code blocks and tables
selected_text = clean_markup(selected_text)  # Remove markdown formatting if needed
selected_text = preserve_structure(selected_text)  # Keep code blocks intact
```

**Case 3: Answer Not in Selected Text**

```python
if not validator.is_response_in_context(response, selected_text):
    return {
        "response": "The selected text does not contain the answer.",
        "in_selected_text": False
    }
```

---

## Section 8: Deployment Guide

### Prerequisites

- OpenAI API key (paid account with credits).
- Qdrant Cloud free tier account and API key.
- Neon Postgres free tier account and connection string.
- Python 3.10+ installed locally.
- Docker installed (for containerization).
- Git and GitHub account.

### Step-by-Step Deployment

#### Step 1: Clone Repository & Set Up Backend

```bash
# Clone repository
git clone https://github.com/yourusername/physical-ai-textbook.git
cd physical-ai-textbook/rag-backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Step 2: Configure Environment Variables

Create a `.env` file in `rag-backend/`:

```env
# OpenAI API
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx

# Qdrant Cloud
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=qdrant_api_key_here

# Neon Postgres
DATABASE_URL=postgresql://user:password@ep-xxxxx.neon.tech/dbname

# FastAPI
FASTAPI_ENV=production
API_SECRET_KEY=your_secret_api_key_here
ALLOWED_ORIGINS=["https://yourdomain.com"]

# RAG Configuration
RAG_MODEL=gpt-4o
RAG_TEMPERATURE=0.3
RETRIEVAL_K=5
HALLUCINATION_THRESHOLD=0.6

# Rate Limiting
RATE_LIMIT_QUERIES_PER_MIN=10
RATE_LIMIT_QUERIES_PER_DAY=1000
```

#### Step 3: Initialize Database

```bash
# Run migrations using Alembic
alembic upgrade head

# Verify tables created
psql $DATABASE_URL -c "\dt"
```

#### Step 4: Test Locally

```bash
# Start FastAPI dev server
uvicorn main:app --reload --port 8000

# In another terminal, test endpoints
curl http://localhost:8000/health

# Test ingestion (if you have sample chapters)
python ingest_chapters.py --book-version v1.0
```

#### Step 5: Dockerize Backend

Create `Dockerfile` in `rag-backend/`:

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENV PYTHONUNBUFFERED=1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and push image:

```bash
docker build -t rag-chatbot:latest .
docker tag rag-chatbot:latest yourusername/rag-chatbot:latest
docker push yourusername/rag-chatbot:latest
```

#### Step 6: Deploy to Serverless Host (Render/Railway Example)

**Using Render**:

1. Sign up at https://render.com.
2. Create new Web Service.
3. Connect GitHub repo.
4. Configure:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn main:app --host 0.0.0.0 --port 8000`
5. Add environment variables (copy from `.env`).
6. Deploy.

**Deployment URL**: `https://rag-chatbot-xxxxx.onrender.com`

#### Step 7: Integrate with Docusaurus Frontend

In `docusaurus_textbook/`:

```bash
# Add chat component
cp rag-backend/sdk/chat-embed.js docusaurus_textbook/static/

# Configure API endpoint in Docusaurus config
# docusaurus.config.js
const config = {
  // ...
  customFields: {
    ragChatbotAPI: process.env.RAG_API_URL || 'http://localhost:8000',
  },
};

# Add chat component to theme
# docusaurus_textbook/src/theme/DocLayout/index.js
import RagChatbot from '../components/RagChatbot';

// In layout JSX
<RagChatbot apiUrl={config.customFields.ragChatbotAPI} />
```

#### Step 8: Test End-to-End

```bash
# 1. Verify backend is running
curl https://your-rag-api.onrender.com/health

# 2. Test query endpoint
curl -X POST https://your-rag-api.onrender.com/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is ROS 2?"}'

# 3. Start Docusaurus and open book
cd docusaurus_textbook
npm start

# 4. Open chat sidebar and ask a question
```

#### Step 9: Set Up Monitoring & Alerts

**Application Logging**:

```python
# In main.py
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('rag-chatbot.log')
    ]
)
logger = logging.getLogger(__name__)
```

**Monitoring Tools**:
- **Uptime**: Use UptimeRobot (free tier) to monitor `/health` endpoint.
- **Error Alerts**: Set up email alerts for 5xx errors.
- **Cost Tracking**: Monitor OpenAI API dashboard weekly.

#### Step 10: Scaling Considerations

**If Qdrant Free Tier is Exceeded**:
```
1. Upgrade to Qdrant paid tier: https://qdrant.io/pricing
2. Or: Archive old book versions to separate collections
```

**If Neon Free Tier is Exceeded**:
```
1. Upgrade to Neon paid tier: https://neon.tech/pricing
2. Or: Archive old chat messages (>1 year) to cold storage
```

**If OpenAI Costs Spike**:
```
1. Implement response caching (Redis or Neon).
2. Use GPT-3.5-turbo fallback during high load.
3. Set monthly budget alerts in OpenAI dashboard.
```

---

## Section 9: Limitations & Future Improvements

### Current Limitations

1. **Single Book Version**: Currently supports one book version at a time. Multi-version support would require namespace management in Qdrant.

2. **Static Content**: Book content is loaded at ingest time. Real-time content updates require re-ingestion.

3. **No Fine-Tuning**: Uses base OpenAI models; custom fine-tuning on textbook domain would improve accuracy.

4. **Limited Context Window**: LLM context limited to ~2000 tokens; very long questions may truncate context.

5. **No Multilingual Support**: Currently English-only. Urdu translation (mentioned in project spec) would require additional prompts/models.

6. **Conversation Context**: Multi-turn conversations don't leverage previous context beyond message history. Advanced techniques (conversation summarization, context chaining) not implemented.

7. **No User Authentication**: Sessions are anonymous. User accounts would enable personalized learning profiles.

8. **Limited Explainability**: Responses cite sources but don't explain retrieval/scoring rationale. Explainability features would improve educational value.

### Future Improvements

#### Short Term (v1.1-v1.2)

1. **Conversation Summarization**:
   - Summarize long conversations to reduce context size.
   - Enable context carryover across multiple queries.

2. **Advanced Re-ranking**:
   - Implement cross-encoder models for better relevance scoring.
   - Use BM25 + semantic hybrid search.

3. **Response Caching**:
   - Cache frequent queries and responses.
   - Reduce latency and OpenAI costs by 20-30%.

4. **User Feedback Loop**:
   - Allow readers to rate responses (ğŸ‘/ğŸ‘).
   - Use feedback to improve re-ranking and generation.

#### Medium Term (v1.5-v2.0)

1. **Multi-Version Book Support**:
   - Manage multiple book versions simultaneously.
   - Enable readers to compare versions.
   - Auto-migrate conversations to new versions.

2. **Question Clarification**:
   - If query is ambiguous, ask clarifying follow-ups.
   - Reduce out-of-scope responses by 50%.

3. **Knowledge Graph Integration**:
   - Extract and build knowledge graph from book content.
   - Enable structured query answering ("What modules cover ROS 2?").

4. **Personalized Learning**:
   - Track reader interaction and comprehension.
   - Recommend relevant sections based on questions.
   - Adaptive difficulty levels.

5. **Multilingual Support**:
   - Support Urdu, Spanish, Mandarin translations.
   - Automatic translation of queries and responses.

6. **Real-Time Collaborative Features**:
   - Share conversations with instructors.
   - Enable instructor feedback on student questions.

#### Long Term (v3.0+)

1. **Custom Fine-Tuned Models**:
   - Fine-tune embedding and generation models on textbook domain.
   - Improve accuracy from 90% to 95%+.

2. **Autonomous Agent Capabilities**:
   - Enable chatbot to perform multi-step reasoning.
   - Answer questions requiring synthesis across multiple chapters.

3. **Integration with External Resources**:
   - Link to video tutorials, code repositories, academic papers.
   - Provide context from beyond the textbook.

4. **Hands-On Tutorial Generation**:
   - Generate code examples and tutorials based on questions.
   - Enable interactive learning exercises.

5. **Real-Time Content Updates**:
   - Detect and ingest chapter updates automatically.
   - Versioned embeddings for different content states.

### Metrics for Success

As these improvements are implemented, track:

- **RAG Accuracy**: Percentage of correct answers (target: 95%+).
- **User Engagement**: Number of queries/day, avg conversation length.
- **Cost Efficiency**: Cost per query, cost trends.
- **User Satisfaction**: Feedback ratings, NPS score.
- **Learning Outcomes**: Correlation between chatbot usage and exam performance (if applicable).

---

## Conclusion

The Integrated RAG Chatbot transforms a static textbook into an interactive, intelligent learning platform. By combining semantic search (Qdrant), LLM generation (OpenAI), and strict RAG constraints, the system delivers accurate, sourced answers while preventing hallucination.

The architecture is production-ready, scalable to thousands of concurrent users, and cost-efficient using free-tier cloud services (~$15-60/month). The three-agent pipeline (retrieval, re-ranking, generation) provides a clean, maintainable design that can evolve with future improvements.

For educators and learners, the chatbot offers a new paradigm for interactive technical educationâ€”one where students can ask questions, explore concepts deeply, and learn at their own pace, all within the context of authoritative textbook content.

---

## References

- **OpenAI API Documentation**: https://platform.openai.com/docs
- **Qdrant Cloud**: https://qdrant.io/
- **Neon Postgres**: https://neon.tech/
- **FastAPI**: https://fastapi.tiangolo.com/
- **Retrieval-Augmented Generation (RAG) Papers**:
  - Lewis et al. (2020): "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
  - Izacard & Grave (2020): "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"
