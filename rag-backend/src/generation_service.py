"""
Content Generation Service
Generates responses using LLM with RAG context.
"""

import logging
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from src.config import get_settings

logger = logging.getLogger(__name__)

try:
    from openai import OpenAI, RateLimitError, APIConnectionError
except ImportError:
    logger.warning("OpenAI not installed; install with: pip install openai")


@dataclass
class GeneratedResponse:
    """Response generated by LLM."""

    content: str
    model: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    stop_reason: str = "stop"

    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "content": self.content,
            "model": self.model,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "total_tokens": self.total_tokens,
            "stop_reason": self.stop_reason,
        }


class PromptTemplates:
    """Prompt templates for different query modes."""

    @staticmethod
    def rag_prompt(query: str, context: str) -> str:
        """
        RAG prompt template for retrieval-augmented generation.

        Args:
            query: User question
            context: Retrieved context from vector DB

        Returns:
            Formatted prompt string
        """
        return f"""You are a helpful educational assistant specializing in robotics and ROS 2.

Use the provided context to answer the user's question accurately and thoroughly.
If the context doesn't contain relevant information, say so clearly.
Provide clear, concise explanations with practical examples where appropriate.

Context from educational materials:
{context}

User Question: {query}

Answer:"""

    @staticmethod
    def system_message() -> str:
        """System message for all queries."""
        return """You are an expert educational assistant specializing in robotics, ROS 2, and AI systems.
Your role is to:
1. Provide accurate, well-structured answers based on provided context
2. Explain concepts clearly for learners at various levels
3. Include practical examples and code snippets when relevant
4. Acknowledge when information is outside the provided context
5. Suggest related topics for further learning

Be concise but comprehensive. Use markdown formatting for clarity."""

    @staticmethod
    def selected_text_prompt(query: str, context: str, selected_text: str) -> str:
        """
        Prompt for selected-text mode (user highlighted specific content).

        Args:
            query: User question
            context: Retrieved context
            selected_text: Text the user highlighted

        Returns:
            Formatted prompt string
        """
        return f"""You are a helpful educational assistant specializing in robotics and ROS 2.

The user highlighted this specific text from their materials:
---
{selected_text}
---

Using this highlighted text and additional context, answer their question:

Additional Context:
{context}

User Question: {query}

Answer:"""

    @staticmethod
    def clarification_prompt(query: str) -> str:
        """
        Prompt when no sufficient context is available.

        Args:
            query: User question

        Returns:
            Formatted prompt string
        """
        return f"""You are an expert educational assistant specializing in robotics and ROS 2.

The user asked this question, but we don't have specific context from course materials for it:

Question: {query}

Based on your knowledge, provide a helpful answer. Be clear that this is general knowledge
rather than from the specific course materials being studied.

Answer:"""


class GenerationAgent:
    """Generates responses using OpenAI LLM with RAG context."""

    def __init__(
        self,
        model: str = "gpt-4o",
        fallback_model: str = "gpt-3.5-turbo",
        temperature: float = 0.3,
        max_tokens: int = 500,
    ):
        """
        Initialize generation agent.

        Args:
            model: Primary LLM model
            fallback_model: Fallback model if primary fails
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens in response
        """
        self.model = model
        self.fallback_model = fallback_model
        self.temperature = temperature
        self.max_tokens = max_tokens

        try:
            settings = get_settings()

            # Check if OpenAI API key is available
            if not settings.openai_api_key or settings.openai_api_key == "sk-proj-default":
                logger.warning("‚ö†Ô∏è OpenAI API key not configured, generation service will return context-based responses only")
                self.client = None
                return

            # Use OpenAI API with the key from environment variables
            self.client = OpenAI(api_key=settings.openai_api_key)
            logger.info("‚úÖ OpenAI client initialized for generation service")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize OpenAI client: {e}")
            self.client = None

        self.token_usage = {"input": 0, "output": 0, "total": 0}

    def generate(
        self,
        query: str,
        context: str,
        mode: str = "full_book",
        selected_text: Optional[str] = None,
        retry_count: int = 0,
    ) -> GeneratedResponse:
        """
        Generate response using LLM with RAG context.

        Args:
            query: User question
            context: Retrieved context from vector store
            mode: "full_book" or "selected_text"
            selected_text: Optional highlighted text for selected_text mode
            retry_count: Internal retry counter

        Returns:
            GeneratedResponse with content and token usage

        Raises:
            Exception: If generation fails
        """
        if not self.client:
            # Return a response based on the context without calling OpenAI
            logger.warning("‚ö†Ô∏è OpenAI API key not configured, generating response from context only")
            content = f"Based on the provided context: {context}\n\nQuestion: {query}\n\n[Note: OpenAI API key not configured - this is a context-based response without LLM generation]"
            return GeneratedResponse(
                content=content,
                model="context-only",
                input_tokens=0,
                output_tokens=0,
                total_tokens=0,
            )

        try:
            # Select prompt template based on mode
            if mode == "selected_text" and selected_text:
                prompt = PromptTemplates.selected_text_prompt(
                    query, context, selected_text
                )
            else:
                prompt = PromptTemplates.rag_prompt(query, context)

            logger.info(
                f"ü§ñ Generating response (model={self.model}, temp={self.temperature})"
            )

            # Call LLM
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": PromptTemplates.system_message(),
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )

            # Extract response
            content = response.choices[0].message.content
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens

            # Track usage
            self.token_usage["input"] += input_tokens
            self.token_usage["output"] += output_tokens
            self.token_usage["total"] += total_tokens

            logger.info(
                f"‚úÖ Response generated: {output_tokens} tokens "
                f"(model={self.model})"
            )

            return GeneratedResponse(
                content=content,
                model=self.model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                total_tokens=total_tokens,
            )

        except RateLimitError as e:
            logger.warning(f"‚ö†Ô∏è Rate limit hit, retrying (attempt {retry_count + 1})")
            if retry_count < 3:
                import time

                wait_time = 2 ** (retry_count + 1)
                time.sleep(wait_time)
                return self.generate(
                    query,
                    context,
                    mode=mode,
                    selected_text=selected_text,
                    retry_count=retry_count + 1,
                )
            raise

        except APIConnectionError as e:
            logger.error(f"‚ùå API connection error: {e}")
            # Try fallback model
            if self.model != self.fallback_model:
                logger.info(f"üîÑ Trying fallback model: {self.fallback_model}")
                original_model = self.model
                self.model = self.fallback_model
                try:
                    result = self.generate(query, context, mode, selected_text)
                    self.model = original_model
                    return result
                except Exception as fallback_error:
                    self.model = original_model
                    raise fallback_error
            raise

        except Exception as e:
            logger.error(f"‚ùå Generation failed: {e}", exc_info=True)
            # Return a fallback response when API fails
            content = f"Based on the provided context: {context}\n\nQuestion: {query}\n\n[Note: LLM generation failed - this is a context-based response]"
            return GeneratedResponse(
                content=content,
                model="fallback",
                input_tokens=0,
                output_tokens=0,
                total_tokens=0,
            )

    def get_cost_estimate(self) -> Dict:
        """
        Calculate estimated cost of token usage.

        OpenAI pricing (as of model training):
        - GPT-4o: $0.005 per 1K input tokens, $0.015 per 1K output tokens
        - GPT-3.5-turbo: $0.0005 per 1K input tokens, $0.0015 per 1K output tokens

        Returns:
            Dict with token counts and estimated cost
        """
        # Use model-specific pricing
        if "gpt-4o" in self.model.lower():
            input_cost_per_1k = 0.005
            output_cost_per_1k = 0.015
        else:
            # GPT-3.5-turbo and others
            input_cost_per_1k = 0.0005
            output_cost_per_1k = 0.0015

        input_cost = (
            self.token_usage["input"] / 1000 * input_cost_per_1k
        )
        output_cost = (
            self.token_usage["output"] / 1000 * output_cost_per_1k
        )
        total_cost = input_cost + output_cost

        return {
            "input_tokens": self.token_usage["input"],
            "output_tokens": self.token_usage["output"],
            "total_tokens": self.token_usage["total"],
            "input_cost_usd": round(input_cost, 4),
            "output_cost_usd": round(output_cost, 4),
            "total_cost_usd": round(total_cost, 4),
        }

    def reset_token_counter(self):
        """Reset token usage counter."""
        self.token_usage = {"input": 0, "output": 0, "total": 0}
        logger.info("üîÑ Token counter reset")


# Singleton instance
_generation_agent_instance = None


def get_generation_agent(
    model: str = "gpt-4o",
    fallback_model: str = "gpt-3.5-turbo",
    temperature: float = 0.3,
    max_tokens: int = 500,
) -> GenerationAgent:
    """Get or create generation agent instance."""
    global _generation_agent_instance
    if _generation_agent_instance is None:
        _generation_agent_instance = GenerationAgent(
            model=model,
            fallback_model=fallback_model,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    return _generation_agent_instance
